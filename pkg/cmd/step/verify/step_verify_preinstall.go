package verify

import (
	"fmt"
	"io/ioutil"
	"net"
	"os"
	"path/filepath"
	"strings"

	"github.com/blang/semver"
	v1 "github.com/jenkins-x/jx/pkg/apis/jenkins.io/v1"
	"github.com/jenkins-x/jx/pkg/versionstream"

	"github.com/jenkins-x/jx/pkg/cloud/amazon/session"
	"github.com/jenkins-x/jx/pkg/prow"
	"sigs.k8s.io/yaml"

	"github.com/jenkins-x/jx/pkg/boot"
	"github.com/jenkins-x/jx/pkg/cloud"
	"github.com/jenkins-x/jx/pkg/cloud/amazon"
	"github.com/jenkins-x/jx/pkg/cloud/buckets"
	"github.com/jenkins-x/jx/pkg/cloud/factory"
	"github.com/jenkins-x/jx/pkg/cloud/gke"
	"github.com/jenkins-x/jx/pkg/cmd/create"
	"github.com/jenkins-x/jx/pkg/cmd/helper"
	"github.com/jenkins-x/jx/pkg/cmd/namespace"
	"github.com/jenkins-x/jx/pkg/cmd/opts"
	"github.com/jenkins-x/jx/pkg/cmd/opts/step"
	"github.com/jenkins-x/jx/pkg/config"
	"github.com/jenkins-x/jx/pkg/gits"
	"github.com/jenkins-x/jx/pkg/io/secrets"
	"github.com/jenkins-x/jx/pkg/kube"
	"github.com/jenkins-x/jx/pkg/kube/cluster"
	"github.com/jenkins-x/jx/pkg/kube/naming"
	"github.com/jenkins-x/jx/pkg/log"
	"github.com/jenkins-x/jx/pkg/util"
	"github.com/pkg/errors"
	"github.com/spf13/cobra"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
)

const (
	defaultSecretsYaml = `secrets:     
  adminUser:
    username: "admin"
    password: "" 
  hmacToken: "" 
  pipelineUser:
    username: ""  
    email: "" 
    token: ""
`
)

// StepVerifyPreInstallOptions contains the command line flags
type StepVerifyPreInstallOptions struct {
	StepVerifyOptions
	Debug                  bool
	Dir                    string
	LazyCreate             bool
	DisableVerifyHelm      bool
	DisableVerifyPackages  bool
	DefaultHelmfileSecrets bool
	LazyCreateFlag         string
	Namespace              string
	ProviderValuesDir      string
	TestKanikoSecretData   string
	TestVeleroSecretData   string
	WorkloadIdentity       bool
	NoSecretYAMLValidate   bool
}

// NewCmdStepVerifyPreInstall creates the `jx step verify pod` command
func NewCmdStepVerifyPreInstall(commonOpts *opts.CommonOptions) *cobra.Command {

	options := &StepVerifyPreInstallOptions{
		StepVerifyOptions: StepVerifyOptions{
			StepOptions: step.StepOptions{
				CommonOptions: commonOpts,
			},
		},
	}

	cmd := &cobra.Command{
		Use:     "preinstall",
		Aliases: []string{"pre-install", "pre"},
		Short:   "Verifies all of the cloud infrastructure is setup before we try to boot up a cluster via 'jx boot'",
		Run: func(cmd *cobra.Command, args []string) {
			options.Cmd = cmd
			options.Args = args
			err := options.Run()
			helper.CheckErr(err)
		},
	}
	cmd.Flags().BoolVarP(&options.Debug, "debug", "", false, "Output logs of any failed pod")
	cmd.Flags().StringVarP(&options.Dir, "dir", "d", ".", "the directory to look for the install requirements file")
	cmd.Flags().StringVarP(&options.LazyCreateFlag, "lazy-create", "", "", fmt.Sprintf("Specify true/false as to whether to lazily create missing resources. If not specified it is enabled if Terraform is not specified in the %s file", config.RequirementsConfigFileName))
	cmd.Flags().StringVarP(&options.Namespace, "namespace", "", "", "the namespace that Jenkins X will be booted into. If not specified it defaults to $DEPLOY_NAMESPACE")
	cmd.Flags().StringVarP(&options.ProviderValuesDir, "provider-values-dir", "", "", "The optional directory of kubernetes provider specific files")
	cmd.Flags().BoolVarP(&options.WorkloadIdentity, "workload-identity", "", false, "Enable this if using GKE Workload Identity to avoid reconnecting to the Cluster.")
	cmd.Flags().BoolVarP(&options.DisableVerifyPackages, "disable-verify-packages", "", false, "Disable packages verification, helpful when testing different package versions.")
	cmd.Flags().BoolVarP(&options.DisableVerifyHelm, "disable-verify-helm", "", false, "Disable Helm verification, helpful when testing different Helm versions.")
	cmd.Flags().BoolVarP(&options.DefaultHelmfileSecrets, "default-helmfile-secrets", "", false, "If we are in a Pull Request and using helmfile we may want to generate default secrets if they are not yet present so we can lint the helmfile.")

	return cmd
}

// Run implements this command
func (o *StepVerifyPreInstallOptions) Run() error {
	info := util.ColorInfo
	requirements, requirementsFileName, err := config.LoadRequirementsConfig(o.Dir, config.DefaultFailOnValidationError)
	if err != nil {
		return err
	}

	if requirements.Helmfile && !o.NoSecretYAMLValidate {
		err = o.validateSecretsYAML()
		if err != nil {
			return err
		}
	}

	err = o.ConfigureCommonOptions(requirements)
	if err != nil {
		return err
	}

	requirements, err = o.gatherRequirements(requirements, requirementsFileName)
	if err != nil {
		return err
	}

	err = o.ValidateRequirements(requirements, requirementsFileName)
	if err != nil {
		return err
	}

	o.LazyCreate, err = requirements.IsLazyCreateSecrets(o.LazyCreateFlag)
	if err != nil {
		return err
	}

	// lets find the namespace to use
	if o.Namespace == "" {
		o.Namespace = requirements.Cluster.Namespace
	}
	ns, err := o.GetDeployNamespace(o.Namespace)
	if err != nil {
		return err
	}
	kubeClient, err := o.KubeClient()
	if err != nil {
		return err
	}

	err = o.verifyTLS(requirements)
	if err != nil {
		return errors.WithStack(err)
	}

	o.SetDevNamespace(ns)

	log.Logger().Infof("Verifying the kubernetes cluster before we try to boot Jenkins X in namespace: %s", info(ns))
	if o.LazyCreate {
		log.Logger().Infof("Trying to lazily create any missing resources to get the current cluster ready to boot Jenkins X")
	} else {
		log.Logger().Warn("Lazy create of cloud resources is disabled")
	}

	err = o.verifyDevNamespace(kubeClient, ns)
	if err != nil {
		if o.LazyCreate {
			log.Logger().Infof("Attempting to lazily create the deploy namespace %s", info(ns))

			err = kube.EnsureDevNamespaceCreatedWithoutEnvironment(kubeClient, ns)
			if err != nil {
				return errors.Wrapf(err, "failed to lazily create the namespace %s", ns)
			}
			// lets rerun the verify step to ensure its all sorted now
			err = o.verifyDevNamespace(kubeClient, ns)
			if err != nil {
				return errors.Wrapf(err, "failed to verify the namespace %s", ns)
			}
		}
	}

	err = o.verifyIngress(requirements, requirementsFileName)
	if err != nil {
		return err
	}
	no := &namespace.NamespaceOptions{}
	no.CommonOptions = o.CommonOptions
	no.Args = []string{ns}
	err = no.Run()
	if err != nil {
		return err
	}
	log.Logger().Info("\n")

	po := &StepVerifyPackagesOptions{}
	po.CommonOptions = o.CommonOptions
	po.Packages = []string{"kubectl", "git", "helm"}
	po.Dir = o.Dir
	if !o.DisableVerifyPackages {
		err = po.Run()
		if err != nil {
			return err
		}
		log.Logger().Info("\n")
	}

	err = o.VerifyInstallConfig(kubeClient, ns, requirements, requirementsFileName)
	if err != nil {
		return err
	}
	err = o.verifyStorage(requirements, requirementsFileName)
	if err != nil {
		return err
	}
	log.Logger().Info("\n")
	if !o.DisableVerifyHelm && !requirements.Helmfile {
		err = o.verifyHelm(ns)
		if err != nil {
			return err
		}
	}

	if vns := requirements.Velero.Namespace; vns != "" {
		if requirements.Cluster.Provider == cloud.GKE {
			log.Logger().Infof("Validating the velero secret in namespace %s", info(vns))

			err = o.validateVelero(vns)
			if err != nil {
				if o.LazyCreate {
					log.Logger().Infof("Attempting to lazily create the deploy namespace %s", info(vns))

					err = o.lazyCreateVeleroSecret(requirements, vns)
					if err != nil {
						return errors.Wrapf(err, "failed to lazily create the velero secret in: %s", vns)
					}
					// lets rerun the verify step to ensure its all sorted now
					err = o.validateVelero(vns)
				}
			}
			if err != nil {
				return err
			}
			log.Logger().Info("\n")
		}
	}

	if requirements.Webhook == config.WebhookTypeLighthouse {
		// we don't need the ConfigMaps for prow yet
		err = o.verifyProwConfigMaps(kubeClient, ns)
		if err != nil {
			return err
		}
	}

	if requirements.Cluster.Provider == cloud.EKS && o.LazyCreate {
		if !cluster.IsInCluster() || os.Getenv("OVERRIDE_IRSA_IN_CLUSTER") == "true" {
			log.Logger().Info("Attempting to lazily create the IAM Role for Service Accounts permissions")
			err = amazon.EnableIRSASupportInCluster(requirements)
			if err != nil {
				return errors.Wrap(err, "error enabling IRSA in cluster")
			}
			if o.ProviderValuesDir == "" {
				// lets default to the version stream
				ec, err := o.EnvironmentContext(o.Dir, true)
				if err != nil {
					return errors.Wrapf(err, "failed to create EnvironmentContext")
				}
				versionResolver := ec.VersionResolver
				if versionResolver == nil {
					return fmt.Errorf("no VersionResolver")
				}
				o.ProviderValuesDir = filepath.Join(versionResolver.VersionsDir, "kubeProviders")
				log.Logger().Infof("using the directory %s for EKS templates", util.ColorInfo(o.ProviderValuesDir))
			}
			err = amazon.CreateIRSAManagedServiceAccounts(requirements, o.ProviderValuesDir)
			if err != nil {
				return errors.Wrap(err, "error creating the IRSA managed Service Accounts")
			}
		} else {
			log.Logger().Info("Running in cluster, not recreating permissions")
		}
	}

	// Lets update the TeamSettings with the VersionStream data from the jx-requirements.yml file so we make sure
	// we are upgrading with the latest versions
	log.Logger().Infof("Cluster looks good, you are ready to '%s' now!", info("jx boot"))
	fmt.Println()
	return nil
}

// EnsureHelm ensures helm is installed
func (o *StepVerifyPreInstallOptions) verifyHelm(ns string) error {
	log.Logger().Debug("Verifying Helm...")
	// lets make sure we don't try use tiller
	o.EnableRemoteKubeCluster()
	v, err := o.Helm().Version(false)
	if err != nil {
		err = o.InstallHelm()
		if err != nil {
			return errors.Wrap(err, "failed to install Helm")
		}
		v, err = o.Helm().Version(false)
		if err != nil {
			return errors.Wrap(err, "failed to get Helm version after install")
		}
	}
	currVersion, err := semver.Make(v)
	if err != nil {
		return errors.Wrapf(err, "unable to parse semantic version %s", v)
	}
	noInitRequiredVersion := semver.MustParse("3.0.0")
	if currVersion.LT(noInitRequiredVersion) {
		cfg := opts.InitHelmConfig{
			Namespace:       ns,
			OnlyHelmClient:  true,
			Helm3:           false,
			SkipTiller:      true,
			GlobalTiller:    false,
			TillerNamespace: "",
			TillerRole:      "",
		}
		err = o.InitHelm(cfg)
		if err != nil {
			return errors.Wrapf(err, "initializing helm with config: %v", cfg)
		}
	}

	o.EnableRemoteKubeCluster()

	_, err = o.AddHelmBinaryRepoIfMissing(kube.DefaultChartMuseumURL, kube.DefaultChartMuseumJxRepoName, "", "")
	if err != nil {
		return errors.Wrapf(err, "adding '%s' helm charts repository", kube.DefaultChartMuseumURL)
	}
	log.Logger().Infof("Ensuring Helm chart repository %s is configured\n", kube.DefaultChartMuseumURL)

	return nil
}

func (o *StepVerifyPreInstallOptions) verifyDevNamespace(kubeClient kubernetes.Interface, ns string) error {
	log.Logger().Debug("Verifying Dev Namespace...")
	ns, envName, err := kube.GetDevNamespace(kubeClient, ns)
	if err != nil {
		return err
	}
	if ns == "" {
		return fmt.Errorf("no dev namespace name found")
	}
	if envName == "" {
		return fmt.Errorf("namespace %s has no team label", ns)
	}
	return nil
}

func (o *StepVerifyPreInstallOptions) lazyCreateKanikoSecret(requirements *config.RequirementsConfig, ns string) error {
	log.Logger().Debugf("Lazily creating the kaniko secret")
	io := &create.InstallOptions{}
	io.CommonOptions = o.CommonOptions
	io.Flags.Kaniko = true
	io.Flags.Namespace = ns
	io.Flags.Provider = requirements.Cluster.Provider
	io.SetInstallValues(map[string]string{
		kube.ClusterName: requirements.Cluster.ClusterName,
		kube.ProjectID:   requirements.Cluster.ProjectID,
	})
	if o.TestKanikoSecretData != "" {
		io.AdminSecretsService.Flags.KanikoSecret = o.TestKanikoSecretData
	} else {
		err := io.ConfigureKaniko()
		if err != nil {
			return err
		}
	}
	data := io.AdminSecretsService.Flags.KanikoSecret
	if data == "" {
		return fmt.Errorf("failed to create the kaniko secret data")
	}
	return o.createSecret(ns, kube.SecretKaniko, kube.SecretKaniko, data)
}

func (o *StepVerifyPreInstallOptions) lazyCreateVeleroSecret(requirements *config.RequirementsConfig, ns string) error {
	log.Logger().Debugf("Lazily creating the velero secret")
	var data string
	var err error
	if o.TestVeleroSecretData != "" {
		data = o.TestVeleroSecretData
	} else {
		data, err = o.configureVelero(requirements)
		if err != nil {
			return errors.Wrap(err, "failed to create the velero secret data")
		}
	}
	if data == "" {
		return nil
	}
	return o.createSecret(ns, kube.SecretVelero, "cloud", data)
}

// ConfigureVelero configures the velero SA and secret
func (o *StepVerifyPreInstallOptions) configureVelero(requirements *config.RequirementsConfig) (string, error) {
	if requirements.Cluster.Provider != cloud.GKE {
		log.Logger().Infof("we are assuming your IAM roles are setup so that Velero has cluster-admin\n")
		return "", nil
	}

	serviceAccountDir, err := ioutil.TempDir("", "gke")
	if err != nil {
		return "", errors.Wrap(err, "creating a temporary folder where the service account will be stored")
	}
	defer os.RemoveAll(serviceAccountDir)

	clusterName := requirements.Cluster.ClusterName
	projectID := requirements.Cluster.ProjectID
	if projectID == "" || clusterName == "" {
		if kubeClient, ns, err := o.KubeClientAndDevNamespace(); err == nil {
			if data, err := kube.ReadInstallValues(kubeClient, ns); err == nil && data != nil {
				if projectID == "" {
					projectID = data[kube.ProjectID]
				}
				if clusterName == "" {
					clusterName = data[kube.ClusterName]
				}
			}
		}
	}
	if projectID == "" {
		projectID, err = o.GetGoogleProjectID("")
		if err != nil {
			return "", errors.Wrap(err, "getting the GCP project ID")
		}
		requirements.Cluster.ProjectID = projectID
	}
	if clusterName == "" {
		clusterName, err = o.GetGKEClusterNameFromContext()
		if err != nil {
			return "", errors.Wrap(err, "gettting the GKE cluster name from current context")
		}
		requirements.Cluster.ClusterName = clusterName
	}

	serviceAccountName := requirements.Velero.ServiceAccount
	if serviceAccountName == "" {
		serviceAccountName = naming.ToValidNameTruncated(fmt.Sprintf("%s-vo", clusterName), 30)
		requirements.Velero.ServiceAccount = serviceAccountName
	}
	log.Logger().Infof("Configuring Velero service account %s for project %s", util.ColorInfo(serviceAccountName), util.ColorInfo(projectID))
	serviceAccountPath, err := o.GCloud().GetOrCreateServiceAccount(serviceAccountName, projectID, serviceAccountDir, gke.VeleroServiceAccountRoles)
	if err != nil {
		return "", errors.Wrap(err, "creating the service account")
	}

	bucket := requirements.Storage.Backup.URL
	if bucket == "" {
		return "", fmt.Errorf("missing requirements.storage.backup.url")
	}
	err = o.GCloud().ConfigureBucketRoles(projectID, serviceAccountName, bucket, gke.VeleroServiceAccountRoles)
	if err != nil {
		return "", errors.Wrap(err, "associate the IAM roles to the bucket")
	}

	serviceAccount, err := ioutil.ReadFile(serviceAccountPath)
	if err != nil {
		return "", errors.Wrapf(err, "reading the service account from file '%s'", serviceAccountPath)
	}
	return string(serviceAccount), nil
}

// VerifyInstallConfig lets ensure we modify the install ConfigMap with the requirements
func (o *StepVerifyPreInstallOptions) VerifyInstallConfig(kubeClient kubernetes.Interface, ns string, requirements *config.RequirementsConfig, requirementsFileName string) error {
	log.Logger().Debug("Verifying Install Config...")
	_, err := kube.DefaultModifyConfigMap(kubeClient, ns, kube.ConfigMapNameJXInstallConfig,
		func(configMap *corev1.ConfigMap) error {
			secretsLocation := string(secrets.FileSystemLocationKind)
			if requirements.SecretStorage == config.SecretStorageTypeVault {
				secretsLocation = string(secrets.VaultLocationKind)
			}
			modifyMapIfNotBlank(configMap.Data, kube.KubeProvider, requirements.Cluster.Provider)
			modifyMapIfNotBlank(configMap.Data, kube.ProjectID, requirements.Cluster.ProjectID)
			modifyMapIfNotBlank(configMap.Data, kube.ClusterName, requirements.Cluster.ClusterName)
			modifyMapIfNotBlank(configMap.Data, secrets.SecretsLocationKey, secretsLocation)
			modifyMapIfNotBlank(configMap.Data, kube.Region, requirements.Cluster.Region)
			modifyMapIfNotBlank(configMap.Data, kube.Zone, requirements.Cluster.Zone)
			return nil
		}, nil)
	if err != nil {
		return errors.Wrapf(err, "saving secrets location in ConfigMap %s in namespace %s", kube.ConfigMapNameJXInstallConfig, ns)
	}
	return nil
}

// getIPAddress return the preferred outbound ip of this machine
func getIPAddress() (string, error) {
	conn, err := net.Dial("udp", "8.8.8.8:80")
	if err != nil {
		return "", err
	}
	defer conn.Close()

	localAddr := conn.LocalAddr().(*net.UDPAddr)

	return localAddr.IP.String(), nil
}

// gatherRequirements gathers cluster requirements and connects to the cluster if required
func (o *StepVerifyPreInstallOptions) gatherRequirements(requirements *config.RequirementsConfig, requirementsFileName string) (*config.RequirementsConfig, error) {
	log.Logger().Debug("Gathering Requirements...")
	if o.BatchMode {
		msg := "please specify '%s' in jx-requirements when running  in  batch mode"
		if requirements.Cluster.Provider == "" {
			return nil, errors.Errorf(msg, "provider")
		}
		if requirements.Cluster.Provider == cloud.EKS || requirements.Cluster.Provider == cloud.AWS {
			if requirements.Cluster.Region == "" {
				return nil, errors.Errorf(msg, "region")
			}
		}
		if requirements.Cluster.Provider == cloud.GKE {
			if requirements.Cluster.ProjectID == "" {
				return nil, errors.Errorf(msg, "project")
			}
			if requirements.Cluster.Zone == "" {
				return nil, errors.Errorf(msg, "zone")
			}
		}
		if requirements.Cluster.EnvironmentGitOwner == "" {
			return nil, errors.Errorf(msg, "environmentGitOwner")
		}
		if requirements.Cluster.ClusterName == "" {
			return nil, errors.Errorf(msg, "clusterName")
		}
	}
	var err error
	if requirements.Cluster.Provider == "" {
		requirements.Cluster.Provider, err = util.PickName(cloud.KubernetesProviders, "Select Kubernetes provider", "the type of Kubernetes installation", o.GetIOFileHandles())
		if err != nil {
			return nil, errors.Wrap(err, "selecting Kubernetes provider")
		}
	}

	if requirements.Cluster.Provider != cloud.GKE && requirements.Cluster.Provider != cloud.EKS {
		// lets check we want to try installation as we've only tested on GKE at the moment
		if answer, err := o.showProvideFeedbackMessage(); err != nil {
			return requirements, err
		} else if !answer {
			return requirements, errors.New("finishing execution")
		}
	}

	if requirements.Cluster.Provider == cloud.GKE {
		var currentProject, currentZone, currentClusterName string
		autoAcceptDefaults := false
		if requirements.Cluster.ProjectID == "" || requirements.Cluster.Zone == "" || requirements.Cluster.ClusterName == "" {
			kubeConfig, _, err := o.Kube().LoadConfig()
			if err != nil {
				return nil, errors.Wrapf(err, "loading kubeconfig")
			}
			context := kube.Cluster(kubeConfig)
			currentProject, currentZone, currentClusterName, err = gke.ParseContext(context)
			if err != nil {
				return nil, errors.Wrapf(err, "")
			}
			if currentClusterName != "" && currentProject != "" && currentZone != "" {
				log.Logger().Infof("Currently connected cluster is %s in %s in project %s", util.ColorInfo(currentClusterName), util.ColorInfo(currentZone), util.ColorInfo(currentProject))
				autoAcceptDefaults, err = util.Confirm(fmt.Sprintf("Do you want to jx boot the %s cluster?", util.ColorInfo(currentClusterName)), true, "Enter Y to use the currently connected cluster or enter N to specify a different cluster", o.GetIOFileHandles())
				if err != nil {
					return nil, err
				}
			} else {
				log.Logger().Infof("Enter the cluster you want to jx boot")
			}
		}

		if requirements.Cluster.ProjectID == "" {
			if autoAcceptDefaults && currentProject != "" {
				requirements.Cluster.ProjectID = currentProject
			} else {
				requirements.Cluster.ProjectID, err = o.GetGoogleProjectID(currentProject)
				if err != nil {
					return nil, errors.Wrap(err, "getting project ID")
				}
			}
		}
		if requirements.Cluster.Zone == "" {
			if autoAcceptDefaults && currentZone != "" {
				requirements.Cluster.Zone = currentZone
			} else {
				requirements.Cluster.Zone, err = o.GetGoogleZone(requirements.Cluster.ProjectID, currentZone)
				if err != nil {
					return nil, errors.Wrap(err, "getting GKE Zone")
				}
			}
		}
		if requirements.Cluster.ClusterName == "" {
			if autoAcceptDefaults && currentClusterName != "" {
				requirements.Cluster.ClusterName = currentClusterName
			} else {
				requirements.Cluster.ClusterName, err = util.PickValue("Cluster name", currentClusterName, true,
					"The name for your cluster", o.GetIOFileHandles())
				if err != nil {
					return nil, errors.Wrap(err, "getting cluster name")
				}
				if requirements.Cluster.ClusterName == "" {
					return nil, errors.Errorf("no cluster name provided")
				}
			}
		}
		if requirements.Cluster.Registry == "" {
			requirements.Cluster.Registry = "gcr.io"
		}
		if !autoAcceptDefaults {
			if !o.WorkloadIdentity && !o.InCluster() {
				// connect to the specified cluster if different from the currently connected one
				log.Logger().Infof("Connecting to cluster %s", util.ColorInfo(requirements.Cluster.ClusterName))
				err = o.GCloud().ConnectToCluster(requirements.Cluster.ProjectID, requirements.Cluster.Zone, requirements.Cluster.ClusterName)
				if err != nil {
					return nil, err
				}
			} else {
				log.Logger().Info("no need to reconnect to cluster")
			}
		}
	} else if requirements.Cluster.Provider == cloud.EKS || requirements.Cluster.Provider == cloud.AWS {
		var currentRegion, currentClusterName string
		var autoAcceptDefaults bool
		if requirements.Cluster.Region == "" || requirements.Cluster.ClusterName == "" {
			currentClusterName, currentRegion, err = session.GetCurrentlyConnectedRegionAndClusterName()
			if err != nil {
				return requirements, errors.Wrap(err, "there was a problem obtaining the current cluster name and region")
			}
			if currentClusterName != "" && currentRegion != "" {
				log.Logger().Infof("")
				log.Logger().Infof("Currently connected cluster is %s in region %s", util.ColorInfo(currentClusterName), util.ColorInfo(currentRegion))
				autoAcceptDefaults, err = util.Confirm(fmt.Sprintf("Do you want to jx boot the %s cluster?", util.ColorInfo(currentClusterName)), true, "Enter Y to use the currently connected cluster or enter N to specify a different cluster", o.GetIOFileHandles())
				if err != nil {
					return nil, err
				}
			} else {
				log.Logger().Infof("Enter the cluster you want to jx boot")
			}
		}

		if requirements.Cluster.Region == "" {
			if autoAcceptDefaults && currentRegion != "" {
				requirements.Cluster.Region = currentRegion
			}
		}
		if requirements.Cluster.ClusterName == "" {
			if autoAcceptDefaults && currentClusterName != "" {
				requirements.Cluster.ClusterName = currentClusterName
			} else {
				requirements.Cluster.ClusterName, err = util.PickValue("Cluster name", currentClusterName, true,
					"The name for your cluster", o.GetIOFileHandles())
				if err != nil {
					return nil, errors.Wrap(err, "getting cluster name")
				}
			}
		}
	}

	if requirements.Cluster.ClusterName == "" && !o.BatchMode {
		requirements.Cluster.ClusterName, err = util.PickValue("Cluster name", "", true,
			"The name for your cluster", o.GetIOFileHandles())
		if err != nil {
			return nil, errors.Wrap(err, "getting cluster name")
		}
		if requirements.Cluster.ClusterName == "" {
			return nil, errors.Errorf("no cluster name provided")
		}
	}

	requirements.Cluster.Provider = strings.TrimSpace(strings.ToLower(requirements.Cluster.Provider))
	requirements.Cluster.ProjectID = strings.TrimSpace(requirements.Cluster.ProjectID)
	requirements.Cluster.Zone = strings.TrimSpace(strings.ToLower(requirements.Cluster.Zone))
	requirements.Cluster.Region = strings.TrimSpace(strings.ToLower(requirements.Cluster.Region))
	requirements.Cluster.ClusterName = strings.TrimSpace(strings.ToLower(requirements.Cluster.ClusterName))

	err = o.gatherGitRequirements(requirements)
	if err != nil {
		return nil, errors.Wrap(err, "error gathering git requirements")
	}

	// Lock the version stream to a tag
	if requirements.VersionStream.Ref == "" {
		requirements.VersionStream.Ref = os.Getenv(boot.VersionsRepoBaseRefEnvVarName)
	}
	if requirements.VersionStream.URL == "" {
		requirements.VersionStream.URL = os.Getenv(boot.VersionsRepoURLEnvVarName)
	}

	// attempt to resolve the version stream ref to a tag
	versionStreamDir, ref, err := o.CloneJXVersionsRepo(requirements.VersionStream.URL, requirements.VersionStream.Ref)
	if err != nil {
		return nil, errors.Wrapf(err, "resolving version stream ref")
	}
	if ref != "" && ref != requirements.VersionStream.Ref {
		log.Logger().Infof("Locking version stream %s to release %s. Jenkins X will use this release rather than %s to resolve all versions from now on.", util.ColorInfo(requirements.VersionStream.URL), util.ColorInfo(ref), requirements.VersionStream.Ref)
		requirements.VersionStream.Ref = ref
	}

	if requirements.BuildPackURL == "" {
		requirements.BuildPackURL = v1.KubernetesWorkloadBuildPackURL
	}
	if requirements.BuildPackRef == "" || requirements.BuildPackRef == "master" {
		// lets resolve the version from the version stream
		resolver := &versionstream.VersionResolver{
			VersionsDir: versionStreamDir,
		}
		gitVersion, err := resolver.StableVersionNumber(versionstream.KindGit, requirements.BuildPackURL)
		if err != nil {
			return nil, errors.Wrapf(err, "failed to resolve git version of %s in the version stream at dir %s", requirements.BuildPackURL, versionStreamDir)
		}
		if gitVersion != "" {
			requirements.BuildPackRef = gitVersion
			log.Logger().Infof("setting the build pack %s to version %s", requirements.BuildPackURL, gitVersion)
		} else {
			log.Logger().Warnf("the version stream at %s does not have a stable git version for %s", versionStreamDir, requirements.BuildPackURL)
		}
	}

	err = o.SaveConfig(requirements, requirementsFileName)
	if err != nil {
		return nil, errors.Wrap(err, "error saving requirements file")
	}

	err = o.writeOwnersFile(requirements)
	if err != nil {
		return nil, errors.Wrapf(err, "writing approvers to OWNERS file in %s", o.Dir)
	}

	return requirements, nil
}

func (o *StepVerifyPreInstallOptions) writeOwnersFile(requirements *config.RequirementsConfig) error {
	if len(requirements.Cluster.DevEnvApprovers) > 0 {
		path := filepath.Join(o.Dir, "OWNERS")
		filename, err := filepath.Abs(path)
		if err != nil {
			return errors.Wrapf(err, "failed to resolve path %s", path)
		}
		data := prow.Owners{}
		for _, approver := range requirements.Cluster.DevEnvApprovers {
			data.Approvers = append(data.Approvers, approver)
			data.Reviewers = append(data.Reviewers, approver)
		}
		ownersYaml, err := yaml.Marshal(data)
		if err != nil {
			return err
		}
		err = ioutil.WriteFile(filename, ownersYaml, 0644)
		if err != nil {
			return err
		}
		log.Logger().Infof("writing the following to the OWNERS file for the development environment repository:\n%s", string(ownersYaml))
	}
	return nil
}

func (o *StepVerifyPreInstallOptions) gatherGitRequirements(requirements *config.RequirementsConfig) error {
	requirements.Cluster.EnvironmentGitOwner = strings.TrimSpace(requirements.Cluster.EnvironmentGitOwner)

	// lets fix up any missing or incorrect git kinds for public git servers
	if gits.IsGitHubServerURL(requirements.Cluster.GitServer) {
		requirements.Cluster.GitKind = "github"
	} else if gits.IsGitLabServerURL(requirements.Cluster.GitServer) {
		requirements.Cluster.GitKind = "gitlab"
	}

	var err error
	if requirements.Cluster.EnvironmentGitOwner == "" {
		requirements.Cluster.EnvironmentGitOwner, err = util.PickValue(
			"Git Owner name for environment repositories",
			"",
			true,
			"Jenkins X leverages GitOps to track and control what gets deployed into environments.  "+
				"This requires a Git repository per environment. "+
				"This question is asking for the Git Owner where these repositories will live.",
			o.GetIOFileHandles())
		if err != nil {
			return errors.Wrap(err, "error configuring git owner for env repositories")
		}

		if requirements.Cluster.EnvironmentGitPublic {
			log.Logger().Infof("Environment repos will be %s, if you want to create %s environment repos, please set %s to %s jx-requirements.yml", util.ColorInfo("public"), util.ColorInfo("private"), util.ColorInfo("environmentGitPublic"), util.ColorInfo("false"))
		} else {
			log.Logger().Infof("Environment repos will be %s, if you want to create %s environment repos, please set %s to %s in jx-requirements.yml", util.ColorInfo("private"), util.ColorInfo("public"), util.ColorInfo("environmentGitPublic"), util.ColorInfo("true"))
		}
	}
	if len(requirements.Cluster.DevEnvApprovers) == 0 && !o.BatchMode {
		approversString, err := util.PickValue(
			"Comma-separated git provider usernames of approvers for development environment repository",
			"",
			true,
			"Pull requests to the development environment repository require approval by one or more "+
				"users, specified in the 'OWNERS' file in the repository. Please specify a comma-separated "+
				"list of usernames for your Git provider to be used as approvers.",
			o.GetIOFileHandles())
		if err != nil {
			return errors.Wrap(err, "configuring approvers for development environment repository")
		}
		for _, a := range strings.Split(approversString, ",") {
			requirements.Cluster.DevEnvApprovers = append(requirements.Cluster.DevEnvApprovers, strings.TrimSpace(a))
		}
	}
	return nil
}

// verifyStorage verifies the associated buckets exist or if enabled lazily create them
func (o *StepVerifyPreInstallOptions) verifyStorage(requirements *config.RequirementsConfig, requirementsFileName string) error {
	log.Logger().Info("Verifying Storage...")
	storage := &requirements.Storage
	err := o.verifyStorageEntry(requirements, requirementsFileName, &storage.Logs, "logs", "Long term log storage")
	if err != nil {
		return err
	}
	err = o.verifyStorageEntry(requirements, requirementsFileName, &storage.Reports, "reports", "Long term report storage")
	if err != nil {
		return err
	}
	err = o.verifyStorageEntry(requirements, requirementsFileName, &storage.Repository, "repository", "Chart repository")
	if err != nil {
		return err
	}
	err = o.verifyStorageEntry(requirements, requirementsFileName, &storage.Backup, "backup", "backup storage")
	if err != nil {
		return err
	}
	log.Logger().Infof("Storage configuration looks good\n")
	return nil
}

func (o *StepVerifyPreInstallOptions) verifyTLS(requirements *config.RequirementsConfig) error {
	if !requirements.Ingress.TLS.Enabled {
		confirm := false
		if requirements.SecretStorage == config.SecretStorageTypeVault {
			log.Logger().Warnf("Vault is enabled and TLS is not enabled. This means your secrets will be sent to and from your cluster in the clear. See %s for more information", config.TLSDocURL)
			confirm = true
		}
		if requirements.Webhook != config.WebhookTypeNone {
			log.Logger().Warnf("TLS is not enabled so your webhooks will be called using HTTP. This means your webhook secret will be sent to your cluster in the clear. See %s for more information", config.TLSDocURL)
			confirm = true
		}
		if os.Getenv(boot.OverrideTLSWarningEnvVarName) == "true" {
			confirm = false
		}
		if confirm && !o.BatchMode {

			message := fmt.Sprintf("Do you wish to continue?")
			help := fmt.Sprintf("Jenkins X needs TLS enabled to send secrets securely. We strongly recommend enabling TLS.")
			if answer, err := util.Confirm(message, false, help, o.GetIOFileHandles()); err != nil {
				return err
			} else if !answer {
				return errors.Errorf("cannot continue because TLS is not enabled.")
			}
		}

	}
	return nil
}

func (o *StepVerifyPreInstallOptions) verifyStorageEntry(requirements *config.RequirementsConfig, requirementsFileName string, storageEntryConfig *config.StorageEntryConfig, name string, text string) error {
	kubeProvider := requirements.Cluster.Provider
	if !storageEntryConfig.Enabled {
		if requirements.IsCloudProvider() {
			log.Logger().Warnf("Your requirements have not enabled cloud storage for %s - we recommend enabling this for kubernetes provider %s", name, kubeProvider)
		}
		return nil
	}

	provider := factory.NewBucketProvider(requirements)

	if storageEntryConfig.URL == "" {
		// lets allow the storage bucket to be entered or created
		if o.BatchMode {
			log.Logger().Warnf("No URL provided for storage: %s", name)
			return nil
		}
		scheme := buckets.KubeProviderToBucketScheme(kubeProvider)
		if scheme == "" {
			scheme = "s3"
		}
		message := fmt.Sprintf("%s bucket URL. Press enter to create and use a new bucket", text)
		help := fmt.Sprintf("please enter the URL of the bucket to use for storage using the format %s://<bucket-name>", scheme)
		value, err := util.PickValue(message, "", false, help, o.GetIOFileHandles())
		if err != nil {
			return errors.Wrapf(err, "failed to pick storage bucket for %s", name)
		}

		if value == "" {
			if provider == nil {
				log.Logger().Warnf("the kubernetes provider %s has no BucketProvider in jx yet so we cannot lazily create buckets", kubeProvider)
				log.Logger().Warnf("long term storage for %s will be disabled until you provide an existing bucket URL", name)
				return nil
			}
			safeClusterName := naming.ToValidName(requirements.Cluster.ClusterName)
			safeName := naming.ToValidName(name)
			value, err = provider.CreateNewBucketForCluster(safeClusterName, safeName)
			if err != nil {
				return errors.Wrapf(err, "failed to create a dynamic bucket for cluster %s and name %s", safeClusterName, safeName)
			}
		}
		if value != "" {
			storageEntryConfig.URL = value

			err = o.SaveConfig(requirements, requirementsFileName)
			if err != nil {
				return errors.Wrapf(err, "failed to save changes to file: %s", requirementsFileName)
			}
		}
	}

	if storageEntryConfig.URL != "" {
		if provider == nil {
			log.Logger().Warnf("the kubernetes provider %s has no BucketProvider in jx yet - so you have to manually setup and verify your bucket URLs exist", kubeProvider)
			log.Logger().Infof("please verify this bucket exists: %s", util.ColorInfo(storageEntryConfig.URL))
			return nil
		}

		err := provider.EnsureBucketIsCreated(storageEntryConfig.URL)
		if err != nil {
			return errors.Wrapf(err, "failed to ensure the bucket URL %s is created", storageEntryConfig.URL)
		}
	}
	return nil
}

func (o *StepVerifyPreInstallOptions) verifyProwConfigMaps(kubeClient kubernetes.Interface, ns string) error {
	err := o.verifyConfigMapExists(kubeClient, ns, "config", "config.yaml", "pod_namespace: jx")
	if err != nil {
		return err
	}
	return o.verifyConfigMapExists(kubeClient, ns, "plugins", "plugins.yaml", "cat: {}")
}

func (o *StepVerifyPreInstallOptions) verifyConfigMapExists(kubeClient kubernetes.Interface, ns string, name string, key string, defaultValue string) error {
	info := util.ColorInfo
	configMapInterface := kubeClient.CoreV1().ConfigMaps(ns)
	cm, err := configMapInterface.Get(name, metav1.GetOptions{})
	if err != nil {
		// lets try create it
		cm = &corev1.ConfigMap{
			ObjectMeta: metav1.ObjectMeta{
				Name: name,
			},
			Data: map[string]string{
				key: defaultValue,
			},
		}
		cm, err = configMapInterface.Create(cm)
		if err != nil {
			// maybe someone else just created it - lets try one more time
			cm2, err2 := configMapInterface.Get(name, metav1.GetOptions{})
			if err == nil {
				log.Logger().Infof("created ConfigMap %s in namespace %s", info(name), info(ns))
			}
			if err2 != nil {
				return fmt.Errorf("failed to create the ConfigMap %s in namespace %s due to: %s - we cannot get it either: %s", name, ns, err.Error(), err2.Error())
			}
			cm = cm2
			err = nil
		}
	}
	if err != nil {
		return err
	}

	// lets verify that there is an entry
	if cm.Data == nil {
		cm.Data = map[string]string{}
	}
	_, ok := cm.Data[key]
	if !ok {
		cm.Data[key] = defaultValue
		cm.Name = name

		_, err = configMapInterface.Update(cm)
		if err != nil {
			return fmt.Errorf("failed to update the ConfigMap %s in namespace %s to add key %s due to: %s", name, ns, key, err.Error())
		}
	}
	log.Logger().Infof("verified there is a ConfigMap %s in namespace %s", info(name), info(ns))
	return nil
}

func (o *StepVerifyPreInstallOptions) verifyIngress(requirements *config.RequirementsConfig, requirementsFileName string) error {
	log.Logger().Info("Verifying Ingress...")
	domain := requirements.Ingress.Domain

	modified := false
	// if we are discovering the domain name from the ingress service this can change if a cluster/service is recreated
	// so we need to recreate it each time to be sure the IP address is still correct
	if !requirements.Ingress.IgnoreLoadBalancer {
		if requirements.Ingress.IsAutoDNSDomain() && requirements.Ingress.ServiceType != "NodePort" {
			log.Logger().Infof("Clearing the domain %s as when using auto-DNS domains we need to regenerate to ensure its always accurate in case the cluster or ingress service is recreated", util.ColorInfo(domain))
			requirements.Ingress.Domain = ""
			modified = true
		} else if requirements.Ingress.ServiceType == "NodePort" {
			log.Logger().Infof("Clearing the domain %s as we need to ensure we discover the domain from the ingress NodePort and optional node externalIP in case the cluster, node or ingress service is recreated", util.ColorInfo(domain))
			requirements.Ingress.Domain = ""
			modified = true
		}
	}

	switch requirements.Cluster.Provider {
	case cloud.KIND:
		if requirements.Ingress.ServiceType == "" {
			requirements.Ingress.ServiceType = "NodePort"
		}
		requirements.Ingress.IgnoreLoadBalancer = true
		modified = true

		ip, err := getIPAddress()
		if err != nil {
			return err
		}

		if requirements.Cluster.Registry == "" {
			if ip != "" {
				requirements.Cluster.Registry = fmt.Sprintf("%s:5000", ip)
				log.Logger().Infof("defaulting to container registry: %s", util.ColorInfo(requirements.Cluster.Registry))
			} else {
				log.Logger().Info("cannot detect the external IP address of this machine. Please update the requirements cluster.Registry value to the host/IP address and port of your container registry")
			}
		}
		if requirements.Ingress.Domain == "" {
			if ip != "" {
				requirements.Ingress.Domain = fmt.Sprintf("%s.nip.io", ip)
				log.Logger().Infof("defaulting to ingress domain: %s", util.ColorInfo(requirements.Ingress.Domain))
			} else {
				log.Logger().Info("cannot detect the external IP address of this machine. Please update the requirements ingress.domain value to access your ingress controller")
			}
		}
	}

	if modified {
		err := o.SaveConfig(requirements, requirementsFileName)
		if err != nil {
			return errors.Wrapf(err, "failed to save changes to file: %s", requirementsFileName)
		}
	}

	log.Logger().Info("\n")
	return nil
}

// ValidateRequirements validate the requirements; e.g. the webhook and git provider
func (o *StepVerifyPreInstallOptions) ValidateRequirements(requirements *config.RequirementsConfig, fileName string) error {
	if requirements.Webhook == config.WebhookTypeProw {
		kind := requirements.Cluster.GitKind
		server := requirements.Cluster.GitServer
		if (kind != "" && kind != "github") || (server != "" && !gits.IsGitHubServerURL(server)) {
			return fmt.Errorf("invalid requirements in file %s cannot use prow as a webhook for git kind: %s server: %s. Please try using lighthouse instead", fileName, kind, server)
		}
	}
	if requirements.Repository == config.RepositoryTypeBucketRepo && requirements.Cluster.ChartRepository == "" {
		requirements.Cluster.ChartRepository = "http://bucketrepo/bucketrepo/charts/"
		err := o.SaveConfig(requirements, fileName)
		if err != nil {
			return errors.Wrapf(err, "failed to save changes to file: %s", fileName)
		}
	}

	modified := false

	// lets verify that we have a repository name defined for every environment
	for i, env := range requirements.Environments {
		if env.Repository == "" {
			clusterName := requirements.Cluster.ClusterName
			repoName := "environment-" + clusterName

			// we only need to add the env key to the git repository if there is more than one environment
			if len(requirements.Environments) > 1 {
				if clusterName != "" {
					clusterName = clusterName + "-"
				}
				repoName = "environment-" + clusterName + env.Key
			}
			requirements.Environments[i].Repository = naming.ToValidName(repoName)
			modified = true
		}
	}
	if modified {
		err := o.SaveConfig(requirements, fileName)
		if err != nil {
			return errors.Wrapf(err, "failed to save changes to file: %s", fileName)
		}
	}
	return nil
}

// SaveConfig saves the configuration file to the given project directory
func (o *StepVerifyPreInstallOptions) SaveConfig(c *config.RequirementsConfig, fileName string) error {
	data, err := yaml.Marshal(c)
	if err != nil {
		return err
	}
	err = ioutil.WriteFile(fileName, data, util.DefaultWritePermissions)
	if err != nil {
		return errors.Wrapf(err, "failed to save file %s", fileName)
	}

	if c.Helmfile {
		err = config.SaveRequirementsValuesFile(c, filepath.Dir(fileName))
		if err != nil {
			return err
		}
	}
	return nil
}

func modifyMapIfNotBlank(m map[string]string, key string, value string) {
	if m != nil {
		if value != "" {
			m[key] = value
		} else {
			log.Logger().Debugf("Cannot update key %s, value is nil", key)
		}
	} else {
		log.Logger().Debugf("Cannot update key %s, map is nil", key)
	}
}

func (o *StepVerifyPreInstallOptions) showProvideFeedbackMessage() (bool, error) {
	log.Logger().Info("jx boot has only been validated on GKE and EKS, we'd love feedback and contributions for other Kubernetes providers")
	if !o.BatchMode {
		return util.Confirm("Continue execution anyway?",
			true, "", o.GetIOFileHandles())
	}
	log.Logger().Info("Running in Batch Mode, execution will continue")
	return true, nil
}

func (o *StepVerifyPreInstallOptions) validateSecretsYAML() error {
	// lets make sure we have the secrets defined as an env var
	secretsYaml := os.Getenv("JX_SECRETS_YAML")
	if secretsYaml == "" {
		if o.DefaultHelmfileSecrets {
			dir, err := ioutil.TempDir("", "jx-secrets-")
			if err != nil {
				return errors.Wrap(err, "failed to create temp dir for default secrets YAML")
			}
			secretsYaml := filepath.Join(dir, "secrets.yaml")
			os.Setenv("JX_SECRETS_YAML", secretsYaml)
		} else {
			return fmt.Errorf("no $JX_SECRETS_YAML environment variable defined.\nPlease point this at your 'secrets.yaml' file.\nSee https://github.com/jenkins-x/enhancements/blob/master/proposals/2/docs/getting-started.md#setting-up-your-secrets")
		}
	}

	// lets write a default file if it doesn't exist so that we can run things like `helmfile lint` in PR pipelines
	// and it provides users with a file they can edit to fill in easily
	exists, err := util.FileExists(secretsYaml)
	if err != nil {
		return err
	}
	if !exists {
		dir := filepath.Dir(secretsYaml)
		err = os.MkdirAll(dir, util.DefaultWritePermissions)
		if err != nil {
			return errors.Wrapf(err, "failed to ensure secrets dir exists: %s", dir)
		}

		err = ioutil.WriteFile(secretsYaml, []byte(defaultSecretsYaml), util.DefaultFileWritePermissions)
		if err != nil {
			return errors.Wrapf(err, "failed to save default secrets YAML file to : %s", dir)
		}
		log.Logger().Infof("generated a default empty Secrets YAML file at: %s", util.ColorInfo(secretsYaml))
	}

	// TODO lets validate the contents and populate the secrets file?
	return nil
}
